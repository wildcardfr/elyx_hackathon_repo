# streamlit_app.py ‚Äî Elyx Life (Advanced Judge-Ready)
import json
from datetime import datetime, timedelta
from io import StringIO
import base64

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

st.set_page_config(page_title="Elyx Member Journey ‚Äî Judge Ready", layout="wide")
st.title("üèÜ Elyx Life ‚Äî Member Journey (Judge-Ready)")
st.caption("Polished visualizations, automated checklist, narrative insights and exports")

# ---------- Sidebar ----------
with st.sidebar:
    st.header("‚öôÔ∏è Controls")
    uploaded = st.file_uploader("Upload JSON (optional)", type=["json"])
    show_judge = st.checkbox("üéØ Show Judge Mode", value=True)
    show_insights = st.checkbox("üí° Show Auto Insights", value=True)
    st.markdown("---")
    st.caption("Tip: upload your own JSON to test. Use judge mode during demo.")

# ---------- Data Loader ----------
@st.cache_data
def load_data(file_like=None):
    try:
        if file_like:
            data = json.load(file_like)
        else:
            with open("data_full_8months.json", "r") as f:
                data = json.load(f)
    except Exception:
        # fallback empty structure
        data = {}

    # Ensure required keys with safe defaults
    data.setdefault("metrics", [])
    data.setdefault("messages", [])
    data.setdefault("decisions", [])
    data.setdefault("persona", {})
    data.setdefault("internal_metrics", {})

    # DataFrames
    df_metrics = pd.DataFrame(data["metrics"])
    df_msgs = pd.DataFrame(data["messages"])
    df_dec = pd.DataFrame(data["decisions"])

    # Normalize and safe-cast
    for df in (df_metrics, df_msgs, df_dec):
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
    return data, df_metrics, df_msgs, df_dec

data, df_metrics, df_msgs, df_decisions = load_data(uploaded)

# Guardrails
if df_metrics.empty and df_msgs.empty:
    st.error("No usable metrics or messages found. Upload JSON or include `data_full_8months.json` in repo.")
    st.stop()

# ---------- Utility ----------
def safe_pct(n, d):
    return 0.0 if d == 0 else round(100 * n / d, 1)

def download_link_str(obj_str: str, filename: str, mime: str):
    b64 = base64.b64encode(obj_str.encode()).decode()
    return f"data:{mime};base64,{b64}"

# ---------- Derived KPIs ----------
# pick common metric names robustly (lowercase compare)
def metric_series(df, name):
    if df.empty or "metric" not in df.columns or "value" not in df.columns:
        return pd.Series(dtype=float)
    mask = df["metric"].str.lower() == name.lower()
    return df.loc[mask, "value"].astype(float)

bp_series = metric_series(df_metrics, "Blood Pressure")
vo2_series = metric_series(df_metrics, "VO2max")
sleep_series = metric_series(df_metrics, "Sleep Score")

bp_avg = int(bp_series.mean()) if not bp_series.empty else None
bp_ctrl_rate = safe_pct((bp_series <= 130).sum(), bp_series.count()) if not bp_series.empty else 0
vo2_delta = (int(vo2_series.iloc[-1]) - int(vo2_series.iloc[0])) if len(vo2_series) >= 2 else 0
sleep_avg = int(sleep_series.mean()) if not sleep_series.empty else None

# Adherence proxy (weeks with >=4 member messages)
if not df_msgs.empty:
    if "date" in df_msgs.columns:
        df_msgs["date_only"] = pd.to_datetime(df_msgs["date"].dt.date)
    else:
        df_msgs["date_only"] = pd.NaT
    if "role" in df_msgs.columns:
        member_msgs = df_msgs[df_msgs["role"].str.lower().eq("member", na=False)]
    else:
        member_msgs = df_msgs
    # week index by ISO week
    if not member_msgs.empty:
        member_msgs["week"] = member_msgs["date_only"].dt.isocalendar().week
        weeks_count = member_msgs["week"].nunique()
        active_weeks = member_msgs.groupby("week").size().ge(4).sum()
        adherence = safe_pct(active_weeks, weeks_count)
    else:
        adherence = 0
else:
    adherence = 0

# ---------- Header KPIs ----------
k1, k2, k3, k4 = st.columns(4)
k1.metric("Avg BP (mmHg)", value=bp_avg if bp_avg is not None else "‚Äî",
          delta=f"{bp_ctrl_rate}% days ‚â§130")
k2.metric("VO‚ÇÇmax change", value=f"{vo2_delta:+} pts",
          delta="8-month delta" if vo2_delta is not None else "")
k3.metric("Sleep score (avg)", value=sleep_avg if sleep_avg is not None else "‚Äî",
          delta="goal ‚â• 70")
k4.metric("Adherence proxy", value=f"{adherence}%", delta="weeks ‚â•4 chats")

st.markdown("---")

# ---------- Global Filters ----------
# compute min/max across available date columns
date_cols = []
for df in (df_metrics, df_msgs, df_decisions):
    if not df.empty and "date" in df.columns:
        date_cols.append(df["date"].dropna())
min_date = min((s.min() for s in date_cols), default=None)
max_date = max((s.max() for s in date_cols), default=None)

c1, c2, c3 = st.columns([2, 2, 1])
with c1:
    if min_date and max_date:
        date_range = st.date_input("Date range", value=(min_date.date(), max_date.date()))
    else:
        today = datetime.today().date()
        date_range = st.date_input("Date range", value=(today - timedelta(days=7), today))
with c2:
    search_text = st.text_input("Search text (messages & decisions)")
with c3:
    pillar_opts = ["All"]
    try:
        p1 = list(df_msgs.get("pillar", pd.Series([], dtype=object)).dropna().unique())
        p2 = list(df_decisions.get("pillar", pd.Series([], dtype=object)).dropna().unique())
        pillar_opts += sorted(list(set(p1 + p2)))
    except Exception:
        pass
    pillar_filter = st.selectbox("Pillar", pillar_opts)

# Apply filters
def in_range(df, date_range_tuple):
    if df.empty or "date" not in df.columns:
        return df
    start, end = pd.to_datetime(date_range_tuple[0]), pd.to_datetime(date_range_tuple[1])
    return df[(df["date"] >= start) & (df["date"] <= end)]

df_metrics_f = in_range(df_metrics, date_range)
df_msgs_f = in_range(df_msgs, date_range)
df_decisions_f = in_range(df_decisions, date_range)

if pillar_filter != "All":
    if "pillar" in df_msgs_f.columns:
        df_msgs_f = df_msgs_f[df_msgs_f["pillar"] == pillar_filter]
    if "pillar" in df_decisions_f.columns:
        df_decisions_f = df_decisions_f[df_decisions_f["pillar"] == pillar_filter]

if search_text and search_text.strip():
    q = search_text.lower()
    if "message" in df_msgs_f.columns:
        df_msgs_f = df_msgs_f[df_msgs_f["message"].str.lower().str.contains(q, na=False)]
    if not df_decisions_f.empty:
        mask = df_decisions_f.apply(lambda r: q in json.dumps(r.dropna().to_dict()).lower(), axis=1)
        df_decisions_f = df_decisions_f[mask]

# ---------- Tabs ----------
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üìà Metrics Comparison",
    "üí¨ Chats & Engagement",
    "üìã Decision Register",
    "üßë Persona & Ops",
    "‚¨áÔ∏è Exports & Snapshot",
])

# ---------------- Tab 1: Multi-metric comparison ----------------
with tab1:
    st.subheader("Multi-metric comparison & mini insights")
    if df_metrics_f.empty:
        st.info("No metrics to show for selected filters.")
    else:
        # pivot to have each metric as column, group by date
        pivot = (df_metrics_f.groupby(["date", "metric"])["value"]
                 .mean().reset_index()
                 .pivot(index="date", columns="metric", values="value")
                 .sort_index())
        # create figure with multiple traces, put first numeric on left, others optionally secondary axis
        fig = go.Figure()
        metrics_list = list(pivot.columns)
        colors = px.colors.qualitative.Plotly
        for i, m in enumerate(metrics_list):
            # put first as primary axis, others on secondary axis to avoid scale conflicts
            use_secondary = True if i >= 1 else False
            fig.add_trace(go.Scatter(
                x=pivot.index, y=pivot[m],
                name=m,
                mode="lines+markers",
                yaxis="y2" if use_secondary else "y",
                marker=dict(size=6),
                line=dict(width=2, color=colors[i % len(colors)])
            ))
        # layout with secondary y-axis
        fig.update_layout(
            title="Metrics over time (primary + secondary axis)",
            xaxis=dict(title="Date"),
            yaxis=dict(title="Primary"),
            yaxis2=dict(title="Secondary", overlaying="y", side="right"),
            legend=dict(orientation="h", yanchor="bottom", y=-0.25, xanchor="left", x=0),
            margin=dict(t=40, b=50)
        )
        st.plotly_chart(fig, use_container_width=True)

        # Auto-insights (rule-based)
        if show_insights:
            insights = []
            # VO2 change
            if not vo2_series.empty and len(vo2_series) >= 2:
                insights.append(f"VO‚ÇÇmax changed by **{vo2_delta:+} pts** over the window.")
            # BP control
            if bp_avg is not None:
                insights.append(f"Avg BP is **{bp_avg} mmHg** with **{bp_ctrl_rate}%** of readings ‚â§130.")
            # Sleep
            if sleep_avg is not None:
                if sleep_avg >= 70:
                    insights.append(f"Average sleep score **{sleep_avg}** meets the goal (‚â•70).")
                else:
                    insights.append(f"Average sleep score **{sleep_avg}** is below goal (‚â•70); consider sleep interventions.")
            # Adherence
            insights.append(f"Adherence (weeks with ‚â•4 member chats): **{adherence}%**.")

            st.markdown("**Auto insights:**")
            for i, ins in enumerate(insights, 1):
                st.write(f"{i}. {ins}")

# ---------------- Tab 2: Chats & Engagement ----------------
with tab2:
    st.subheader("Chats, activity timeline & engagement heatmap")
    if df_msgs_f.empty:
        st.info("No messages for selected filters.")
    else:
        left, right = st.columns([2, 1])
        with left:
            st.caption(f"{len(df_msgs_f)} messages in selection")
            # show threaded messages (collapsible)
            for _, row in df_msgs_f.sort_values("date").iterrows():
                stamp = row["date"].strftime("%Y-%m-%d %H:%M") if pd.notna(row.get("date")) else ""
                sender = row.get("sender", "Unknown")
                role = row.get("role", "")
                header = f"**{stamp} ‚Äî {sender} ({role})**"
                with st.expander(header, expanded=False):
                    st.write(row.get("message", ""))
                    if row.get("pillar"):
                        st.caption(f"Pillar: {row.get('pillar')}")
        with right:
            # activity by sender chart
            if "sender" in df_msgs_f.columns:
                by_sender = df_msgs_f.groupby("sender").size().reset_index(name="messages")
                fig_s = px.bar(by_sender, x="messages", y="sender", orientation="h", title="Messages by sender", text="messages")
                st.plotly_chart(fig_s, use_container_width=True)

        # calendar heatmap: messages per day matrix by week x weekday
        df_days = df_msgs_f.copy()
        df_days["date_only"] = pd.to_datetime(df_days["date"].dt.date)
        day_counts = df_days.groupby("date_only").size().reset_index(name="count")
        if not day_counts.empty:
            # build continuous range between min and max
            start = day_counts["date_only"].min()
            end = day_counts["date_only"].max()
            all_dates = pd.DataFrame({"date_only": pd.date_range(start, end)})
            merged = all_dates.merge(day_counts, on="date_only", how="left").fillna(0)
            merged["week_of_year"] = merged["date_only"].dt.isocalendar().week
            merged["weekday"] = merged["date_only"].dt.weekday  # 0=Mon
            # map weeks to a readable label (week start date)
            week_labels = merged.groupby("week_of_year")["date_only"].min().sort_values().to_dict()
            merged["week_label"] = merged["week_of_year"].map(lambda w: week_labels.get(w).strftime("%Y-%m-%d"))
            heat = merged.pivot_table(index="week_label", columns="weekday", values="count", fill_value=0)
            # sort weekdays
            weekday_names = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
            heat = heat[[i for i in range(7) if i in heat.columns]]
            # plotly heatmap
            fig_h = go.Figure(data=go.Heatmap(
                z=heat.values,
                x=[weekday_names[i] for i in heat.columns],
                y=heat.index,
                colorscale="Blues",
                hovertemplate="Week: %{y}<br>Day: %{x}<br>Messages: %{z}<extra></extra>"
            ))
            fig_h.update_layout(title="Engagement heatmap (messages/day)", height=350, margin=dict(t=40, b=10))
            st.plotly_chart(fig_h, use_container_width=True)

# ---------------- Tab 3: Decision Register ----------------
with tab3:
    st.subheader("Decisions, traceability & impact")
    if df_decisions_f.empty:
        st.info("No decisions in selected range.")
    else:
        # summary per pillar
        if "pillar" in df_decisions_f.columns:
            by_pillar = df_decisions_f.groupby("pillar").size().reset_index(name="decisions")
            st.plotly_chart(px.bar(by_pillar, x="pillar", y="decisions", title="Decisions by pillar"), use_container_width=True)
        st.markdown("#### Recent decisions (traceability to messages)")
        display_cols = [c for c in ["date", "title", "pillar", "by", "rationale"] if c in df_decisions_f.columns]
        st.dataframe(df_decisions_f[display_cols].sort_values("date", ascending=False), use_container_width=True)

        st.markdown("#### Traceability (click to expand)")
        for _, d in df_decisions_f.sort_values("date", ascending=False).iterrows():
            title = d.get("title", "Untitled")
            pillar = d.get("pillar", "")
            who = d.get("by", "")
            date_str = d["date"].date().isoformat() if pd.notna(d.get("date")) else ""
            header = f"{date_str} ‚Ä¢ {title} ‚Ä¢ {pillar} ‚Ä¢ {who}"
            with st.expander(header):
                st.write(d.get("rationale", ""))
                ids = d.get("related_message_ids", []) or []
                if not isinstance(ids, list):
                    ids = [ids]
                if not ids:
                    st.info("No related message ids recorded.")
                else:
                    for rid in ids:
                        row = df_msgs[df_msgs.get("id", None) == rid] if "id" in df_msgs.columns else pd.DataFrame()
                        if not row.empty:
                            r = row.iloc[0]
                            stamp = r.get("date").strftime("%Y-%m-%d %H:%M") if pd.notna(r.get("date")) else ""
                            st.markdown(f"- **{stamp} ‚Äî {r.get('sender','')}**: {r.get('message','')}")
                        else:
                            st.markdown(f"- Related message id **{rid}** not found in messages dataset.")

# ---------------- Tab 4: Persona & Ops ----------------
with tab4:
    st.subheader("Persona & internal metrics")
    persona = data.get("persona", {}) or {}
    # Persona card
    pcol1, pcol2 = st.columns([1, 3])
    with pcol1:
        # avatar fallback
        avatar = persona.get("avatar_url")
        if avatar:
            st.image(avatar, width=120)
        else:
            st.markdown("üë§", unsafe_allow_html=True)
    with pcol2:
        st.markdown(f"### {persona.get('name','Unnamed')}")
        if persona.get("age"):
            st.write(f"- **Age:** {persona.get('age')}")
        if persona.get("occupation"):
            st.write(f"- **Occupation:** {persona.get('occupation')}")
        if persona.get("goals"):
            st.write(f"- **Goals:** {persona.get('goals')}")
        if persona.get("notes"):
            st.write(persona.get("notes"))

    st.subheader("Internal metrics snapshot")
    im = data.get("internal_metrics", {}) or {}
    if isinstance(im, dict) and im:
        im_df = pd.DataFrame(list(im.items()), columns=["Metric", "Value"])
        st.table(im_df)
    else:
        st.info("No internal metrics provided.")

    # Auto judge checklist with computed checks
    if show_judge:
        st.divider()
        st.subheader("Automated Judge Checklist")
        checks = []
        # realistic timeline: at least 120 days span across metrics or messages
        total_span_days = ((max_date - min_date).days) if min_date and max_date else 0
        checks.append(("Realistic timeline (‚â•120 days)", total_span_days >= 120))
        # message frequency: avg messages per week >=5
        avg_msgs_per_week = 0
        if not df_msgs.empty and "date" in df_msgs.columns:
            weeks = ( (df_msgs["date"].max() - df_msgs["date"].min()).days / 7 ) or 1
            avg_msgs_per_week = len(df_msgs) / weeks if weeks else len(df_msgs)
        checks.append(("Member-initiated messages (avg/wk ‚â•5)", avg_msgs_per_week >= 5))
        # decisions traceability: at least half decisions have related_message_ids
        dec_total = len(df_decisions)
        dec_with_links = df_decisions["related_message_ids"].dropna().apply(lambda x: len(x) if isinstance(x, list) else 1).sum() if dec_total>0 and "related_message_ids" in df_decisions.columns else 0
        checks.append(("Decisions linked to chats", dec_total>0 and dec_with_links>0))
        # multi-metric trends: have at least 2 distinct metrics
        checks.append(("Multi-metric trends", df_metrics["metric"].nunique() >= 2 if not df_metrics.empty else False))
        # filters present
        checks.append(("Filters available (date, pillar, search)", True))
        # uploads + exports
        checks.append(("Upload & export features", True))

        # Render checklist with icons
        for label, ok in checks:
            emoji = "‚úÖ" if ok else "‚ùå"
            st.write(f"{emoji} {label}")

        # Summary score (simple heuristic)
        score = int( (sum(1 for _, ok in checks if ok) / len(checks)) * 100 )
        st.metric("Auto checklist score", f"{score}/100")

# ---------------- Tab 5: Exports & Snapshot ----------------
with tab5:
    st.subheader("Export data and download snapshot")

    # CSV/JSON downloads
    if not df_metrics.empty:
        st.download_button("‚¨áÔ∏è Metrics CSV", df_metrics.to_csv(index=False), "metrics.csv", "text/csv")
    if not df_msgs.empty:
        st.download_button("‚¨áÔ∏è Messages CSV", df_msgs.to_csv(index=False), "messages.csv", "text/csv")
    if not df_decisions.empty:
        st.download_button("‚¨áÔ∏è Decisions CSV", df_decisions.to_csv(index=False), "decisions.csv", "text/csv")

    pretty_json = json.dumps(data, indent=2)
    st.download_button("‚¨áÔ∏è Full JSON", pretty_json, "member_journey.json", "application/json")

    # HTML snapshot (lightweight report)
    def make_html_snapshot():
        html = ["<html><head><meta charset='utf-8'><title>Elyx Snapshot</title></head><body>"]
        html.append(f"<h1>Elyx Journey ‚Äî Snapshot</h1>")
        html.append(f"<p>Generated: {datetime.utcnow().isoformat()} UTC</p>")
        html.append("<h2>KPIs</h2><ul>")
        html.append(f"<li>Avg BP: {bp_avg}</li>")
        html.append(f"<li>BP control ‚â§130: {bp_ctrl_rate}%</li>")
        html.append(f"<li>VO2 delta: {vo2_delta}</li>")
        html.append(f"<li>Sleep avg: {sleep_avg}</li>")
        html.append(f"<li>Adherence proxy: {adherence}%</li>")
        html.append("</ul>")
        html.append("<h2>Persona</h2>")
        html.append(f"<pre>{json.dumps(persona, indent=2)}</pre>")
        html.append("<h2>Top insights</h2>")
        if show_insights:
            html.append("<ul>")
            if not vo2_series.empty and len(vo2_series) >= 2:
                html.append(f"<li>VO2 changed by {vo2_delta:+} pts</li>")
            if bp_avg is not None:
                html.append(f"<li>Avg BP {bp_avg} mmHg ({bp_ctrl_rate}% ‚â§130)</li>")
            if sleep_avg is not None:
                html.append(f"<li>Sleep avg {sleep_avg}</li>")
            html.append("</ul>")
        html.append("</body></html>")
        return "\n".join(html)

    snapshot_html = make_html_snapshot()
    st.download_button("‚¨áÔ∏è HTML Snapshot", snapshot_html, "elyx_snapshot.html", "text/html")

st.caption("¬© 2025 Elyx Hackathon ‚Äî advanced dashboard (use judge mode & upload your JSON to demo).")
